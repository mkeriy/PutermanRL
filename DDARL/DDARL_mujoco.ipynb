{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden_size=32, mean_scale=1, min_std=1e-4):\n",
    "        super(Policy, self).__init__()\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_size),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.action = nn.Linear(hidden_size, 2 * out_dim)\n",
    "\n",
    "        self._mean_scale = mean_scale\n",
    "        self._min_std = min_std\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        action = self.action(x)\n",
    "        action_mean, action_std_dev = torch.chunk(action, 2, dim=1)\n",
    "\n",
    "        action_mean = self._mean_scale * torch.tanh(action_mean / self._mean_scale)\n",
    "        action_std_dev = F.softplus(action_std_dev) + self._min_std\n",
    "        return action_mean, action_std_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os, sys\n",
    "import warnings\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "sys.path.append(\"..\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import psutil\n",
    "import gc\n",
    "import gym\n",
    "#import dmc2gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "import torch.optim as optim\n",
    "from itertools import count\n",
    "from gym import spaces\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import glfw\n",
    "from itertools import count\n",
    "import copy\n",
    "\n",
    "from src.networks import FeedForwardNet, FeedForwardNet_v2, EnvModel, TransitionModel, Critic\n",
    "from src.replay_buffer import Replay_buffer\n",
    "from src.utils import fanin_init, weights_init_normal, Average, freeze, unfreeze\n",
    "import src.longenvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_config = {\n",
    "    'max_episodes': 2000,\n",
    "    'max_steps': 5000, # env.steps in one episode\n",
    "    'domain_name': \"Humanoid-v4\",\n",
    "    # 'domain_name': \"HalfCheetah-v4\",\n",
    "    # 'task_name': 'walk',\n",
    "    'seed': 86723146,\n",
    "    'free_runs': 10, # model warm up steps\n",
    "    'terminate_when_unhealthy': False,\n",
    "    'eval_interval': 100\n",
    "}\n",
    "\n",
    "agent_config = {\n",
    "    'policy_update_iterations': 100,\n",
    "    'potential_update_iterations': 100,\n",
    "    'model_update_iterations': 1000,\n",
    "    'critic_update_iteration': 700,\n",
    "    'top_perc': 0.5, # take top_perc of best transitions(state, next state, action, reward, done) by reward\n",
    "    'max_buffer': 100000, # transitions buffer size\n",
    "\n",
    "    'batch_size': 1000,\n",
    "    'lr': 3e-4,\n",
    "    'hidden_size': 512,\n",
    "    'model_hidden_size': 512,\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'max_steps': exp_config['max_steps']\n",
    "}\n",
    "\n",
    "\n",
    "globals().update(exp_config)\n",
    "if terminate_when_unhealthy:\n",
    "    env = gym.make(domain_name)\n",
    "else:\n",
    "    env = gym.make(domain_name, terminate_when_unhealthy=False)\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "    #random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    env.seed(seed)\n",
    "\n",
    "seed_everything(seed)\n",
    "env_reset_rng = np.random.default_rng(seed=3*seed)\n",
    "\n",
    "agent_name = f'{domain_name}_{seed}_k'\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "\n",
    "best_log_score =  0\n",
    "best_log_step = 100\n",
    "\n",
    "directory = './data/'\n",
    "model_directory = directory + 'models/'\n",
    "plot_directory = directory + 'plots/'\n",
    "os.makedirs(directory, exist_ok = True)\n",
    "os.makedirs(model_directory, exist_ok = True)\n",
    "\n",
    "train_ctr = count()\n",
    "eval_ctr = count()\n",
    "\n",
    "run = wandb.init(name=agent_name, project='puterman-rl', save_code=True, config = {**exp_config, **agent_config})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LPModel(object):\n",
    "    def __init__(self, state_dim, action_dim, config_dict):\n",
    "\n",
    "        self.__dict__ = config_dict\n",
    "\n",
    "        self.action_dim = action_dim\n",
    "        self.state_dim = state_dim\n",
    "        self.num_model_update_iteration = 0\n",
    "\n",
    "        # self.policy = FeedForwardNet_v2(state_dim, self.q_dim, action_dim, int(1.5*self.hidden_size)).to(self.device)\n",
    "        self.policy = Policy(state_dim, action_dim, self.hidden_size).to(self.device)\n",
    "        self.policy_optimizer = torch.optim.AdamW(self.policy.parameters(), self.lr)\n",
    "\n",
    "        self.potential = FeedForwardNet(state_dim, action_dim, int(self.hidden_size)).to(self.device)\n",
    "        self.potential_optimizer = torch.optim.AdamW(self.potential.parameters(), self.lr)\n",
    "\n",
    "        self.model = TransitionModel(state_dim, action_dim, 2*self.model_hidden_size, self.model_hidden_size).to(self.device)\n",
    "        self.model_optimizer = optim.AdamW(self.model.parameters(), lr=1e-3)\n",
    "        self.model_loss = nn.MSELoss()\n",
    "\n",
    "\n",
    "        self.critic = Critic(state_dim, action_dim, 2*self.model_hidden_size).to(self.device)\n",
    "        self.critic_optimizer = optim.AdamW(self.critic.parameters(), lr=1e-3)\n",
    "\n",
    "        self.replay_buffer = Replay_buffer(self.max_buffer, top_perc=self.top_perc)\n",
    "\n",
    "    def select_action(self, state, random_prob = 0):\n",
    "        state = Variable(torch.from_numpy(np.float32(state))).reshape(1, -1).to(self.device)\n",
    "        action_mean, action_std_dev = self.policy(state)\n",
    "        action = Normal(action_mean, action_std_dev).rsample()\n",
    "        action = torch.clamp(action, -1, 1)\n",
    "        return action.cpu().data.numpy().flatten()\n",
    "\n",
    "    def critic_update(self):\n",
    "        #start_time = time.time()\n",
    "        closses = []\n",
    "        unfreeze(self.critic);\n",
    "        for it in range(self.critic_update_iteration):\n",
    "            if it % 100 == 0:\n",
    "                x, _, u, r, _ = self.replay_buffer.sample_last(self.max_steps, self.max_steps)\n",
    "            else:\n",
    "                x, _, u, r, _ = self.replay_buffer.sample(self.batch_size)\n",
    "            state = torch.FloatTensor(x).to(self.device)\n",
    "            action = torch.FloatTensor(u).to(self.device)\n",
    "            reward = torch.FloatTensor(r).to(self.device)\n",
    "\n",
    "            current_Q = self.critic(state, action)\n",
    "            critic_loss = F.mse_loss(current_Q, reward)\n",
    "            closses.append(critic_loss)\n",
    "\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            self.critic_optimizer.step()\n",
    "        #print(\"Critic_update: \", str(time.time() - start_time))\n",
    "        return closses\n",
    "\n",
    "    def env_update(self):\n",
    "        #start_time = time.time()\n",
    "        mlosses = []\n",
    "        unfreeze(self.model);\n",
    "        for it in range(self.model_update_iterations):\n",
    "            if it % 100 == 0:\n",
    "                x, y, u, _, _ = self.replay_buffer.sample_last(self.max_steps, self.max_steps)\n",
    "            else:\n",
    "                x, y, u, _, _ = self.replay_buffer.sample(self.batch_size)\n",
    "            state = torch.FloatTensor(x).to(self.device)\n",
    "            action = torch.FloatTensor(u).to(self.device)\n",
    "            next_state = torch.FloatTensor(y).to(self.device)\n",
    "\n",
    "            state_ = self.model(state, action)\n",
    "            loss = self.model_loss(state_, next_state)\n",
    "\n",
    "            mlosses.append(loss)\n",
    "\n",
    "            self.model_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.model_optimizer.step()\n",
    "            self.num_model_update_iteration += 1\n",
    "        #print(\"Env_update: \", str(time.time() - start_time))\n",
    "        return mlosses\n",
    "\n",
    "    def update(self, max_kl=1):\n",
    "        ploss = []\n",
    "        floss = []\n",
    "        prev_step_kl = np.array([0])\n",
    "        prev_policy = copy.deepcopy(self.policy)\n",
    "        for it in range(self.policy_update_iterations):\n",
    "            prev_step_policy = self.policy.state_dict()\n",
    "            prev_step_opt = self.policy_optimizer.state_dict()\n",
    "            \n",
    "            unfreeze(self.policy); freeze(self.potential)#; freeze(self.model); freeze(self.critic)\n",
    "            \n",
    "            #some way of sampling statet and next_states for policy update\n",
    "            if it%5 == 0:\n",
    "                bs = self.max_steps\n",
    "                x, y, _, _, _ = self.replay_buffer.sample_last(bs, self.max_steps)\n",
    "                qs = 20\n",
    "            else:\n",
    "                bs = self.batch_size\n",
    "                x, y, _, _, _ = self.replay_buffer.sample_r_sorted(bs)\n",
    "                qs = 100\n",
    "\n",
    "            sd = agent.state_dim\n",
    "            \n",
    "            next_state_w = torch.FloatTensor(y).to(agent.device).reshape(bs,1,sd).repeat(1,qs,1).reshape(-1, sd)\n",
    "            state = torch.FloatTensor(x).to(agent.device).reshape(bs,1,sd).repeat(1,qs,1).reshape(-1, sd)\n",
    "            action_mean, action_std_dev = self.policy(state)\n",
    "            action = Normal(action_mean, action_std_dev).rsample()\n",
    "            next_state = self.model(state,action)\n",
    "            r = self.critic(state,action)\n",
    "\n",
    "            P_loss = -r.mean() #- self.potential(next_state).mean()\n",
    "            self.policy_optimizer.zero_grad()\n",
    "            P_loss.backward()\n",
    "            self.policy_optimizer.step()\n",
    "            ploss.append(P_loss)\n",
    "\n",
    "\n",
    "        for it in range(self.potential_update_iterations):\n",
    "            freeze(self.policy); unfreeze(self.potential)\n",
    "\n",
    "            gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                action_mean, action_std_dev = self.policy(state)\n",
    "                action = Normal(action_mean, action_std_dev).rsample()\n",
    "                next_state = self.model(state,action)\n",
    "            f_loss = self.potential(next_state_w).mean() - self.potential(next_state.detach()).mean()\n",
    "            f_loss = -f_loss\n",
    "            self.potential_optimizer.zero_grad()\n",
    "            f_loss.backward()\n",
    "            self.potential_optimizer.step()\n",
    "            floss.append(f_loss)\n",
    "            \n",
    "            gc.collect(); torch.cuda.empty_cache()\n",
    "            \n",
    "            \n",
    "            #calculatint KL and adjusting learning rate for stability\n",
    "            with torch.no_grad():\n",
    "                x = self.replay_buffer.sample_last(self.max_steps, self.max_steps)[0]\n",
    "                state = torch.FloatTensor(x).to(self.device)\n",
    "                mean0, std0 = prev_policy(state)\n",
    "                mean1, std1 = self.policy(state)\n",
    "                kl = torch.log(std1) - torch.log(std0) + (std0.pow(2) + (mean0 - mean1).pow(2)) / (2.0 * std1.pow(2)) - 0.5\n",
    "                kl = kl.sum(1, keepdim=True)\n",
    "                gc.collect(); torch.cuda.empty_cache()\n",
    "                \n",
    "                if kl.mean() > max_kl:\n",
    "                    if it > 20:\n",
    "                        self.policy.load_state_dict(prev_step_policy)\n",
    "                        self.policy_optimizer.load_state_dict(prev_step_opt)\n",
    "                        break\n",
    "                    else:\n",
    "                        if kl.mean() > 2*max_kl:\n",
    "                            self.policy.load_state_dict(prev_step_policy)\n",
    "                            self.policy_optimizer.load_state_dict(prev_step_opt)\n",
    "                            self.lr *= 0.8\n",
    "                            for g in self.policy_optimizer.param_groups:\n",
    "                                g['lr'] = self.lr\n",
    "                            for g in self.potential_optimizer.param_groups:\n",
    "                                g['lr'] = self.lr\n",
    "                            break\n",
    "                        else:\n",
    "                            prev_step_kl = kl\n",
    "                            #break\n",
    "                if it > 80:\n",
    "                    self.lr *= 10/8\n",
    "                    for g in self.policy_optimizer.param_groups:\n",
    "                        g['lr'] = self.lr\n",
    "                    for g in self.potential_optimizer.param_groups:\n",
    "                        g['lr'] = self.lr\n",
    "                \n",
    "                prev_step_kl = kl\n",
    "        print(f'Policy updates:\\t {it}, LR:\\t {self.lr}, KL:\\t {prev_step_kl.mean()}')\n",
    "\n",
    "        return action, ploss, floss, prev_step_kl\n",
    "\n",
    "    def evaluate(self, env):\n",
    "        #start_time = time.time()\n",
    "        eval_rewards = []\n",
    "        for i_episode in range(10):\n",
    "            ep_reward = 0\n",
    "            state = env.reset(seed=int(env_reset_rng.integers(np.iinfo(np.int64).max)))\n",
    "            while True:\n",
    "                # env.render()\n",
    "                action = self.select_action(state)\n",
    "                s_, r, done, info = env.step(action)\n",
    "                ep_reward += r\n",
    "                if done:\n",
    "                    break\n",
    "                state = s_\n",
    "            eval_rewards.append(ep_reward)\n",
    "        eval_rewards = np.array(eval_rewards)\n",
    "        min, mean, max = eval_rewards.min(), eval_rewards.mean(), eval_rewards.max()\n",
    "        print(f'Eval: min={min}\\tmean={mean}\\tmax={max}')\n",
    "        return min, mean, max\n",
    "\n",
    "\n",
    "    def save(self):\n",
    "        torch.save(self.policy.state_dict(), directory + 'policy.pth')\n",
    "        torch.save(self.potential.state_dict(), directory + 'potential.pth')\n",
    "        torch.save(self.model.state_dict(), directory + 'model.pth')\n",
    "        print(\"====================================\")\n",
    "        print(\"Model has been saved...\")\n",
    "        print(\"====================================\")\n",
    "\n",
    "    def load(self):\n",
    "        self.policy.load_state_dict(torch.load(directory + 'policy.pth'))\n",
    "        self.potential.load_state_dict(torch.load(directory + 'potential.pth'))\n",
    "        self.model.load_state_dict(torch.load(directory + 'model.pth'))\n",
    "        print(\"====================================\")\n",
    "        print(\"models has been loaded...\")\n",
    "print(\"====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(seed)\n",
    "env_reset_rng = np.random.default_rng(seed=3*seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = LPModel(state_dim, action_dim, agent_config)\n",
    "model_name = agent_name+'_model.pt'\n",
    "policy_name = agent_name+'_best_policy.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.define_metric(\"Steps\")\n",
    "wandb.define_metric(\"*\", step_metric=\"Steps\")\n",
    "\n",
    "total_step = 0\n",
    "ploss, floss, kl = [torch.tensor([0])], [torch.tensor([0])], torch.FloatTensor([0])\n",
    "runs_data = wandb.Table(columns=[\"run_actions\", \"run_rewards\"])\n",
    "all_run_rewards = []\n",
    "\n",
    "for episode in tqdm(range(max_episodes)):\n",
    "    ep_rewards = []\n",
    "    ep_steps = 0\n",
    "    act_tmp = []\n",
    "    rwd_tmp = []\n",
    "    while ep_steps < max_steps:\n",
    "        # Run env interaction\n",
    "        # run_step = 0\n",
    "        run_reward = 0\n",
    "        act_tmp = []\n",
    "        rwd_tmp = []\n",
    "\n",
    "        state = env.reset(seed=int(env_reset_rng.integers(np.iinfo(np.int64).max)))\n",
    "\n",
    "        for run_step in range(1000):#for cheetah 10000\n",
    "            # env.render()\n",
    "            if episode < free_runs:\n",
    "                action = np.random.uniform(\n",
    "                    low=float(env.action_space.low[0]),\n",
    "                    high=float(env.action_space.high[0]),\n",
    "                    size=(action_dim),\n",
    "                )\n",
    "            else:\n",
    "                action = agent.select_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            agent.replay_buffer.push(\n",
    "                (state, next_state, action, reward, np.float32(done))\n",
    "            )\n",
    "\n",
    "            act_tmp.append(action)\n",
    "            rwd_tmp.append(reward)\n",
    "\n",
    "            state = next_state\n",
    "            # run_step += 1\n",
    "            run_reward += reward\n",
    "\n",
    "            total_step += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        ep_steps += run_step\n",
    "        # print(np.array(act_tmp).shape, np.array(rwd_tmp).shape)\n",
    "        runs_data.add_data(np.array(act_tmp), np.array(rwd_tmp))\n",
    "        ep_rewards.append(run_reward)\n",
    "\n",
    "    plt.figure(figsize=(5, 2))\n",
    "    plt.plot(act_tmp[:1000])\n",
    "    plt.show()\n",
    "    print(\"Total T:{} Run Reward: \\t{}\".format(total_step, ep_rewards))\n",
    "\n",
    "    ep_rewards = np.array(ep_rewards)\n",
    "    all_run_rewards.append(ep_rewards)\n",
    "\n",
    "    if ep_rewards.mean() > best_log_score + best_log_step:\n",
    "        best_log_score = ep_rewards.mean()\n",
    "        save_path = f\"{model_directory}best_policy_{run.id}_{episode}_{int(ep_rewards.mean())}.pth\"\n",
    "        torch.save(agent.policy.state_dict(), save_path)\n",
    "        run.log_model(path=save_path)\n",
    "\n",
    "    closs = agent.critic_update()\n",
    "    mloss = agent.env_update()\n",
    "    if episode >= free_runs:\n",
    "        _, ploss, floss, kl = agent.update()\n",
    "\n",
    "    print(\n",
    "        \"Episode: \\t{}  Ploss: \\t{} Floss: \\t{} KL: \\t{} Closs: \\t{} Mloss: \\t{}\".format(\n",
    "            episode,\n",
    "            Average(ploss).item(),\n",
    "            Average(floss).item(),\n",
    "            kl.mean(),\n",
    "            Average(closs).item(),\n",
    "            Average(mloss).item(),\n",
    "        )\n",
    "    )\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"Rollout/min\": ep_rewards.min(),\n",
    "            \"Rollout/mean\": ep_rewards.mean(),\n",
    "            \"Rollout/max\": ep_rewards.max(),\n",
    "            \"Ploss\": Average(ploss).item(),\n",
    "            \"Floss\": Average(floss).item(),\n",
    "            \"KL\": kl.mean(),\n",
    "            \"Closs\": Average(closs).item(),\n",
    "            \"Mloss\": Average(mloss).item(),\n",
    "            \"Episode_data\": runs_data,\n",
    "            \"Steps\": next(train_ctr),\n",
    "        }\n",
    "    )\n",
    "#     if episode % eval_interval == 0:\n",
    "#         #print('Ploss', Average(ploss).item())\n",
    "#         #print('Floss', Average(floss).item())\n",
    "#         eval_min, eval_mean, eval_max = agent.evaluate(env)\n",
    "#         wandb.log({\n",
    "#             'Eval/min': eval_min,\n",
    "#             'Eval/mean': eval_mean,\n",
    "#             'Eval/max': eval_max,\n",
    "#             'Steps': next(eval_ctr)\n",
    "#             })\n",
    "#         #achieved_reward = agent.evaluate(agent.policy, env)\n",
    "#         # if achieved_reward> best_reward:\n",
    "#         #     best_reward = achieved_reward\n",
    "#             #torch.save(agent.policy.state_dict(), f'/home/jovyan/LinearProgrammingRL/saved_models/{policy_name}')\n",
    "#         print(\"--------------------------------\")\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30673,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
